{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radwaahmed20112000/QA-Chatbot/blob/main/Attention_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize"
      ],
      "metadata": {
        "id": "Ie3gRrah6IpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "2X4tluU352jY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFwY_5F6Tyiz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.io import write_file, read_file\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import GRU, LSTM, Bidirectional, Concatenate\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlqXiQqaWgGi",
        "outputId": "b7afcadb-11bd-4b12-9260-a0a58beed371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dYHn0yNOKLO"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = '/content/drive/My Drive/Colab Notebooks/chatbot project/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "LiM-Cn5L6C7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(foldername):\n",
        "  path = SAVE_DIR + foldername + '/0.npy'\n",
        "  return np.load(path)"
      ],
      "metadata": {
        "id": "ApNx0EWxedzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabs = load('vocab')\n",
        "ind = np.where(vocabs == 'START_')\n",
        "print(ind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmUBYo_UZUlz",
        "outputId": "6168e128-968e-4801-9a56-88bbe4015f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([8]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn5pEaVw13L-"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie555-6W16sB"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtWtTJSx01Or"
      },
      "source": [
        "### Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_oGdHPC17_4"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):  \n",
        "  return text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGslvNuwU9LQ"
      },
      "source": [
        "### Deconstruction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GSn2GfqVAx5"
      },
      "outputs": [],
      "source": [
        "def decontracted(phrase):\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocess"
      ],
      "metadata": {
        "id": "cWzGMHhBhwuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  return remove_punctuation(decontracted(sentence.lower()))"
      ],
      "metadata": {
        "id": "gbeKZvvShPDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Global Variables"
      ],
      "metadata": {
        "id": "Amwt2yA855XQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3RM2U8mRDid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8028a4ac-fecf-4ddb-e2c1-659e259775f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "850\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 300\n",
        "units = 150\n",
        "max_length_inp= 58\n",
        "max_length_targ = 218\n",
        "VOCAB_SIZE = 38140\n",
        "BATCH_SIZE = 64\n",
        "length = 54509\n",
        "steps_per_epoch = (length // BATCH_SIZE) - 1\n",
        "print(steps_per_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention Model"
      ],
      "metadata": {
        "id": "4n9_JXIL6MVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder class\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "\n",
        "    # Embed the vocab to a dense embedding \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # GRU Layer\n",
        "    # glorot_uniform: Initializer for the recurrent_kernel weights matrix, \n",
        "    # used for the linear transformation of the recurrent state\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "\n",
        "  # Encoder network comprises an Embedding layer followed by a GRU layer\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  # To initialize the hidden state\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "NLP72Y766SKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(VOCAB_SIZE, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "14tWY2DOEXMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Mechanism\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "pcNSy_eiEbcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(10)"
      ],
      "metadata": {
        "id": "lCYKYvJ-EhdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder class\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # x shape == (batch_size, 1)\n",
        "    # hidden shape == (batch_size, max_length)\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # context_vector shape == (batch_size, hidden_size)\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "metadata": {
        "id": "-vJRckL2EiEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(VOCAB_SIZE, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "RCd8OC7MEk21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize optimizer and loss functions\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# Loss function\n",
        "def loss_function(real, pred):\n",
        "\n",
        "  # Take care of the padding. Not all sequences are of equal length.\n",
        "  # If there's a '0' in the sequence, the loss is being nullified\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "Eiucyt8GEn6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = SAVE_DIR + 'checkpoints/model_attention_LSTM'\n",
        "checkpoint_prefix = os.path.join(path, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "nzjutjYiEsuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = SAVE_DIR + 'checkpoints/model_attention_LSTM/ckpt-41'\n",
        "status = checkpoint.restore(path1)\n",
        "# inside_checkpoint=tf.train.list_variables(tf.train.latest_checkpoint(path1))\n",
        "# print(status.assert_consumed())"
      ],
      "metadata": {
        "id": "Z9VJuaRQGeOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  # tf.GradientTape() -- record operations for automatic differentiation\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # dec_hidden is used by attention, hence is the same enc_hidden\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # <start> token is the initial decoder input\n",
        "    dec_input = tf.expand_dims([8] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "\n",
        "      # Pass enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # Use teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # As this function is called per batch, compute the batch_loss\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  # Get the model's variables\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  # Compute the gradients\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # Update the variables of the model/network\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "Bv6f0bINEtiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 30\n",
        "enc_folder = 'enc_input_data'\n",
        "dec_folder = 'dec_input_data'\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # Initialize the hidden state\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # Loop through the dataset\n",
        "  for batch in range(steps_per_epoch):\n",
        "    inp = np.load(SAVE_DIR + enc_folder + '/' + str(batch) + '.npy')\n",
        "    targ = np.load(SAVE_DIR + dec_folder + '/' + str(batch) + '.npy')\n",
        "    # Call the train method\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "\n",
        "    # Compute the loss (per batch)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # Save (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  # Output the loss observed until that epoch\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  \n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "GcmHO5rLEyGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference"
      ],
      "metadata": {
        "id": "SDTGKxboYE26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate function -- similar to the training loop\n",
        "def evaluate(sentence):\n",
        "\n",
        "  # Attention plot (to be plotted later on) -- initialized with max_lengths of both target and input\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  # Preprocess the sentence given\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  # Fetch the indices concerning the words in the sentence and pad the sequence\n",
        "  inputs = [(np.where(vocabs == i)[0][0]) for i in sentence.split(' ')]\n",
        "  print(inputs)\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  # Convert the inputs to tensors\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([np.where(vocabs == 'START_')[0][0]], 0)\n",
        "\n",
        "  # Loop until the max_length is reached for the target lang (ENGLISH)\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # Store the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    # Get the prediction with the maximum attention\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    # If <end> token is reached, return the result, input, and attention plot\n",
        "    if vocabs[predicted_id] == '_END':\n",
        "      return result, sentence, attention_plot\n",
        "    \n",
        "     # Append the token to the result\n",
        "    result += vocabs[predicted_id] + ' '\n",
        "\n",
        "    # The predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "metadata": {
        "id": "q4WXADUtYItD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(sentence):\n",
        "  result, sentence, _ = evaluate(sentence)\n",
        "\n",
        "  print('question: %s' % (sentence))\n",
        "  print('answer: {}'.format(result))\n",
        "  return result"
      ],
      "metadata": {
        "id": "9Vm_uaWjkejg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"how can I take care of my health?\")"
      ],
      "metadata": {
        "id": "Jh5X3DW8kmEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "def bleu_score(y_true, y_pred):\n",
        "  return sentence_bleu(y_true, y_pred, smoothing_function=SmoothingFunction().method1)"
      ],
      "metadata": {
        "id": "pOoBBaO4HYPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = ['how are you', 'what is the size of this jacket', 'what are the best steps for a daily skincare routine', 'is potato good for kids']"
      ],
      "metadata": {
        "id": "CMJN2UqYHZZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = answer(arr[3])\n",
        "print('bleu score: ', bleu_score(['yes it is good for kids', 'yes', 'yes i would recommend it', 'yes it is full of nutrients'], ans))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZYeU6w3HlUZ",
        "outputId": "04843e38-df6b-4551-d5e6-20923903d1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 11785, 67, 12, 578]\n",
            "question: is potato good for kids\n",
            "answer: no \n",
            "bleu score:  0.14953487812212207\n"
          ]
        }
      ]
    }
  ]
}